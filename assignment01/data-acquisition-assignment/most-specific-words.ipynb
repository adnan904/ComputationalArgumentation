{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import timeit\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#############################################\n",
    "# PLEASE SET TO CORRECT PATH BEFORE RUNNING #\n",
    "#############################################\n",
    "CURRENT_WORKING_DIR = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(\"__file__\")))\n",
    "UNIFIED_DATA_FILE_PATH = f'{CURRENT_WORKING_DIR}/code/data/unified_data.json'\n",
    "TRAIN_TEST_SPLIT_FILE_PATH = f'{CURRENT_WORKING_DIR}/code/data/train-test-split.csv'\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Loading the spaCy Language Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_train_test_split_dict_and_num_essays():\n",
    "    \"\"\"\n",
    "    Reads the train-test-split.csv file and returns a dict {'essayid' : 'split'} and number of essays set as TRAIN\n",
    "    :return: num_essays : number od essays SET to TRAIN in the train-test-split.csv\n",
    "             train_test_split_dict: a dict of the form {'essayid' : 'split'}\n",
    "    \"\"\"\n",
    "    with open(TRAIN_TEST_SPLIT_FILE_PATH, 'r') as train_file:\n",
    "        train_test_split_dict = {}\n",
    "        num_essays = 0\n",
    "        file_content = train_file.read().split('\\n')[1:-1]\n",
    "        for line in file_content:\n",
    "            essay_id = line.split('\";')[0].split('\"essay')[1]\n",
    "            split = line.split(';\"')[1].split('\"')[0]\n",
    "            train_test_split_dict[essay_id] = split\n",
    "            if split == 'TRAIN':\n",
    "                num_essays += 1\n",
    "        return num_essays, train_test_split_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tf_score(document):\n",
    "    words_freq = {}\n",
    "    for k, v in document.items():\n",
    "        text = '. '.join(v)\n",
    "        tokens = nlp(text)\n",
    "        words = [token.text.lower() for token in tokens\n",
    "             if token.is_stop is not True and token.is_punct is not True]\n",
    "        word_count = Counter(words)\n",
    "        tf_scores = {}\n",
    "        for w in word_count:\n",
    "            tf_scores[w] = word_count[w] / len(words)\n",
    "        words_freq[k] = tf_scores\n",
    "    return words_freq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def idf_score(document, tf_score_all_arguments):\n",
    "    idf_scores = {} \n",
    "    for argument in tf_score_all_arguments:\n",
    "        for k in tf_score_all_arguments[argument].keys():\n",
    "            count = sum([k in tf_score_all_arguments[argument_unit] for argument_unit in tf_score_all_arguments])\n",
    "            idf_scores[k] = np.log(len(document)/count)\n",
    "    return idf_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tf_idf_score(document, idf_score_all_arguments, tf_score_all_arguments):\n",
    "    tf_idf_scores_document = {} #score of all argument units such as major-claim, claims, premises  \n",
    "    for argument_unit in document:\n",
    "        tf_idf_scores = {}\n",
    "        for k in tf_score_all_arguments[argument_unit]:\n",
    "            tf_idf_scores[k] = tf_score_all_arguments[argument_unit][k] * idf_score_all_arguments[k]\n",
    "        tf_idf_scores_document[argument_unit] = tf_idf_scores\n",
    "    return  tf_idf_scores_document"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pic_top_10_most_specific_words(tf_idf_scores):\n",
    "    ten_most_common_words = {}\n",
    "    for k, v in tf_idf_scores.items():\n",
    "        ten_most_common_words[k] = Counter(v).most_common(10)\n",
    "    return Counter(ten_most_common_words)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def main():\n",
    "    start = timeit.default_timer()\n",
    "    # Initializing the Statistic Variables\n",
    "    \n",
    "    document = {}\n",
    "    major_claims_text = []\n",
    "    claims_text = []\n",
    "    premises_text = []\n",
    "    # paragraph_text = []\n",
    "    \n",
    "    # Number of Essays = Number of Essays in the train-test-split.csv file that have been SET 'TRAIN'\n",
    "    # Getting the dict of train-test-split of the form {'essayid' : 'split'}\n",
    "    num_of_essays, train_test_split_dict = get_train_test_split_dict_and_num_essays()\n",
    "\n",
    "    with open(UNIFIED_DATA_FILE_PATH, 'r') as f:\n",
    "        unified_file = json.load(f)\n",
    "        for essay in unified_file:\n",
    "            # We only need to compute for essays SET to 'TRAIN'\n",
    "            if train_test_split_dict[essay['id']] == 'TRAIN':\n",
    "                # Tokenizing using the nlp() of the spaCy library\n",
    "                # Appending the text of the argument unit to a list\n",
    "                for major_claim in essay['major_claim']:\n",
    "                    major_claims_text.append(major_claim['text'])\n",
    "                for claim in essay['claims']:\n",
    "                    claims_text.append(claim['text'])\n",
    "                for premise in essay['premises']:\n",
    "                    premises_text.append(premise['text'])\n",
    "                # for paragraph in essay['paragraphs']:\n",
    "                #     paragraph_text.append(premise['text'])\n",
    "                    \n",
    "    document['major_claim'] = major_claims_text\n",
    "    document['claims'] = claims_text\n",
    "    document['premises'] = premises_text\n",
    "    # document['paragraphs'] = premises_text\n",
    "    \n",
    "    tf_score_all_arguments = tf_score(document)\n",
    "    idf_score_all_arguments = idf_score(document, tf_score_all_arguments)\n",
    "    tf_idf_scores = tf_idf_score(document, idf_score_all_arguments, tf_score_all_arguments)\n",
    "    \n",
    "    top_10_most_specific_words = pic_top_10_most_specific_words(tf_idf_scores)\n",
    "        \n",
    "    for element in top_10_most_specific_words:\n",
    "        print(\"\\n10 most specific words in : '{}' \\n \".format(element))\n",
    "        for k, v in top_10_most_specific_words[element]:\n",
    "            print(\"\\n{} : TF-IDF: {}\".format(k, v))\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    print('\\nTime: ', stop - start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "10 most specific words in : 'major_claim' \n",
      " \n",
      "\n",
      "disagree : TF-IDF: 0.002909880851275995\n",
      "\n",
      "scale : TF-IDF: 0.001058138491373089\n",
      "\n",
      "attach : TF-IDF: 0.0007936038685298169\n",
      "\n",
      "enforce : TF-IDF: 0.0007936038685298169\n",
      "\n",
      "compulsory : TF-IDF: 0.0007936038685298169\n",
      "\n",
      "allotted : TF-IDF: 0.0007936038685298169\n",
      "\n",
      "mentioned : TF-IDF: 0.0007936038685298169\n",
      "\n",
      "agree : TF-IDF: 0.0005857911506498884\n",
      "\n",
      "controlling : TF-IDF: 0.0005290692456865445\n",
      "\n",
      "easing : TF-IDF: 0.0005290692456865445\n",
      "\n",
      "10 most specific words in : 'claims' \n",
      " \n",
      "\n",
      "rates : TF-IDF: 0.000762131313678883\n",
      "\n",
      "daily : TF-IDF: 0.0006563199807508731\n",
      "\n",
      "mind : TF-IDF: 0.0005625599835007483\n",
      "\n",
      "cause : TF-IDF: 0.000515679984875686\n",
      "\n",
      "works : TF-IDF: 0.000515679984875686\n",
      "\n",
      "effectiveness : TF-IDF: 0.0005080875424525887\n",
      "\n",
      "vivid : TF-IDF: 0.0005080875424525887\n",
      "\n",
      "convicted : TF-IDF: 0.0005080875424525887\n",
      "\n",
      "related : TF-IDF: 0.0004687999862506236\n",
      "\n",
      "improvement : TF-IDF: 0.00042191998762556134\n",
      "\n",
      "10 most specific words in : 'premises' \n",
      " \n",
      "\n",
      "water : TF-IDF: 0.0008757922918810979\n",
      "\n",
      "example : TF-IDF: 0.000782554123226297\n",
      "\n",
      "writing : TF-IDF: 0.0005531319738196407\n",
      "\n",
      "require : TF-IDF: 0.0005531319738196407\n",
      "\n",
      "favorite : TF-IDF: 0.0005531319738196407\n",
      "\n",
      "local : TF-IDF: 0.0005443854770269892\n",
      "\n",
      "service : TF-IDF: 0.000507037642668004\n",
      "\n",
      "went : TF-IDF: 0.000507037642668004\n",
      "\n",
      "commit : TF-IDF: 0.000507037642668004\n",
      "\n",
      "started : TF-IDF: 0.000507037642668004\n",
      "\n",
      "Time:  11.477560108993202\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}