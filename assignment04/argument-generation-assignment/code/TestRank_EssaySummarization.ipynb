{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "WeptlGXN2MnF",
    "outputId": "781281df-51c3-4729-fe6c-7c8401925582",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adnan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt') # one time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5FvQ9LHGtja",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv('../data/train-test-split.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>essay004</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>essay005</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>essay006</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>essay021</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>essay042</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   SET\n",
       "3   essay004  TEST\n",
       "4   essay005  TEST\n",
       "5   essay006  TEST\n",
       "20  essay021  TEST\n",
       "41  essay042  TEST"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df = csv_df.loc[csv_df['SET'] == 'TEST']\n",
    "csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_essays_id_strings = csv_df['ID']\n",
    "test_essays_ids = [e_id.split('essay')[1] for e_id in test_essays_id_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/essay_prompt_corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "hm6pa5Af4qbE",
    "outputId": "7bf41859-f291-44ae-a785-12cd486a53a1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>Way to reduce the amount of traffic?</td>\n",
       "      <td>Big cities nowadays can be described as meltin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>Qualification is still the fundamental determi...</td>\n",
       "      <td>With the increased educational level of workfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>The government should allocate more funds to p...</td>\n",
       "      <td>The issue of governmental financing provision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198</td>\n",
       "      <td>Improve roads or public transports</td>\n",
       "      <td>Many people think that governments should spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>330</td>\n",
       "      <td>In personal live, we have some responsibilitie...</td>\n",
       "      <td>People should sometimes do things, even though...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                             prompt  \\\n",
       "0  365               Way to reduce the amount of traffic?   \n",
       "1  134  Qualification is still the fundamental determi...   \n",
       "2  131  The government should allocate more funds to p...   \n",
       "3  198                 Improve roads or public transports   \n",
       "4  330  In personal live, we have some responsibilitie...   \n",
       "\n",
       "                                                text  \n",
       "0  Big cities nowadays can be described as meltin...  \n",
       "1  With the increased educational level of workfo...  \n",
       "2  The issue of governmental financing provision ...  \n",
       "3  Many people think that governments should spen...  \n",
       "4  People should sometimes do things, even though...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select only test rows from corpus\n",
    "df = df.loc[df['id'].isin(test_essays_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 3)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_essay_sentences(essay_id, text):\n",
    "    tokenized_essays = {essay_id: sent_tokenize(text)}\n",
    "    return tokenized_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZVoc3R6G9a8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "essay_sentences = [tokenize_essay_sentences(essay_id, text) for essay_id, text in zip(df['id'], df['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/adnan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')# one time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKk_3HZ-Idjm",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX_NFApzIkmC",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "  return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcFap_w9Ivob",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "clean_essay_list = []\n",
    "for essay in essay_sentences:\n",
    "    for k, v in essay.items():\n",
    "        clean_sentences = [remove_stopwords(s.split()) for s in v]\n",
    "        clean_essay = {k: clean_sentences}\n",
    "        clean_essay_list.append(clean_essay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# download pretrained GloVe word embeddings\n",
    "#! wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "K_GXHzwDJq-2",
    "outputId": "9387687a-04f5-41d9-f525-b6bca3b8f97c",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#! unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsXIa7CBKsWQ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('../data/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3VtdSPyKxUZ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vectorized_essays = {}\n",
    "for s in clean_essay_list:\n",
    "    for key, val in s.items():\n",
    "        sentence_vectors = []\n",
    "        for i in val:\n",
    "            if len(i) != 0:\n",
    "                v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "            else:\n",
    "                v = np.zeros((100,))\n",
    "            sentence_vectors.append(v)\n",
    "        vectorized_essays[key] = sentence_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bh9L2pqL3gp"
   },
   "source": [
    "The next step is to find similarities among the sentences. We will use cosine similarity to find similarity between a pair of sentences. Let's create an empty similarity matrix for this task and populate it with cosine similarities of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_fNZpOLxbM",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_sim_matrix(sim_matrix, length_of_sentences, essay_id):\n",
    "    target_essay_sentence_vector = vectorized_essays[essay_id]\n",
    "    for m in range(length_of_sentences):\n",
    "        for j in range(length_of_sentences):\n",
    "            if m != j:\n",
    "                sim_matrix[m][j] = cosine_similarity(target_essay_sentence_vector[m].reshape(1,100), target_essay_sentence_vector[j].reshape(1,100))[0,0]\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find similarities between the sentences of each essay.\n",
    "output = []\n",
    "for e in essay_sentences:\n",
    "    for k, v in e.items():\n",
    "        sim_mat = np.zeros([len(v), len(v)])\n",
    "        sm = create_sim_matrix(sim_mat, len(v), k)\n",
    "        nx_graph = nx.from_numpy_array(sm)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(v)), reverse=True)\n",
    "        # Generate summary\n",
    "        essay_obj = {'id': k, 'prompt': ranked_sentences[0][1]}\n",
    "        output.append(essay_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_dump = json.dumps(output, indent=4, ensure_ascii=False)\n",
    "with open('../data/predictions.json', \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(json_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TestRank_Text_Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
