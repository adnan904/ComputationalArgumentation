{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "WeptlGXN2MnF",
    "outputId": "781281df-51c3-4729-fe6c-7c8401925582",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt') # one time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5FvQ9LHGtja",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "csv_df = pd.read_csv('train-test-split.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csv_df = csv_df.loc[csv_df['SET'] == 'TEST']\n",
    "csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_essays_id_strings = csv_df['ID']\n",
    "test_essays_ids = [e_id.split('essay')[1] for e_id in test_essays_id_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('essay_prompt_corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "hm6pa5Af4qbE",
    "outputId": "7bf41859-f291-44ae-a785-12cd486a53a1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select only test rows from corpus\n",
    "df = df.loc[df['id'].isin(test_essays_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_essay_sentences(essay_id, text):\n",
    "    tokenized_essays = {essay_id: sent_tokenize(text)}\n",
    "    return tokenized_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZVoc3R6G9a8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "essay_sentences = [tokenize_essay_sentences(essay_id, text) for essay_id, text in zip(df['id'], df['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')# one time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKk_3HZ-Idjm",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX_NFApzIkmC",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "  return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcFap_w9Ivob",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "clean_essay_list = []\n",
    "for essay in essay_sentences:\n",
    "    for k, v in essay.items():\n",
    "        clean_sentences = [remove_stopwords(s.split()) for s in v]\n",
    "        clean_essay = {k: clean_sentences}\n",
    "        clean_essay_list.append(clean_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# download pretrained GloVe word embeddings\n",
    "#! wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "K_GXHzwDJq-2",
    "outputId": "9387687a-04f5-41d9-f525-b6bca3b8f97c",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#! unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsXIa7CBKsWQ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3VtdSPyKxUZ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "vectorized_essays = {}\n",
    "for s in clean_essay_list:\n",
    "    for key, val in s.items():\n",
    "        sentence_vectors = []\n",
    "        for i in val:\n",
    "            if len(i) != 0:\n",
    "                v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "            else:\n",
    "                v = np.zeros((100,))\n",
    "            sentence_vectors.append(v)\n",
    "        vectorized_essays[key] = sentence_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bh9L2pqL3gp"
   },
   "source": [
    "The next step is to find similarities among the sentences. We will use cosine similarity to find similarity between a pair of sentences. Let's create an empty similarity matrix for this task and populate it with cosine similarities of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_fNZpOLxbM",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_sim_matrix(sim_matrix, length_of_sentences, essay_id):\n",
    "    target_essay_sentence_vector = vectorized_essays[essay_id]\n",
    "    for m in range(length_of_sentences):\n",
    "        for j in range(length_of_sentences):\n",
    "            if m != j:\n",
    "                sim_matrix[m][j] = cosine_similarity(target_essay_sentence_vector[m].reshape(1,100), target_essay_sentence_vector[j].reshape(1,100))[0,0]\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find similarities between the sentences of each essay.\n",
    "output = []\n",
    "for e in essay_sentences:\n",
    "    for k, v in e.items():\n",
    "        sim_mat = np.zeros([len(v), len(v)])\n",
    "        sm = create_sim_matrix(sim_mat, len(v), k)\n",
    "        nx_graph = nx.from_numpy_array(sm)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(v)), reverse=True)\n",
    "        # Generate summary\n",
    "        essay_obj = {'id': k, 'prompt': ranked_sentences[0][1]}\n",
    "        output.append(essay_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_dump = json.dumps(output, indent=4, ensure_ascii=False)\n",
    "with open('predictions.json', \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(json_dump)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TestRank_Text_Summarization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
